{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content based Movie Recommendation System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "import urllib.request\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_string(my_string):\n",
    "\n",
    "    return re.findall('[\\w\\-]+', my_string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(movies):\n",
    "    \"\"\"\n",
    "    Append a new column to the movies DataFrame with header 'tokens'.\n",
    "    This will contain a list of strings, one per token, extracted\n",
    "    from the 'genre' field of each movie. \n",
    "\n",
    "    Params:\n",
    "      movies...The movies DataFrame\n",
    "    Returns:\n",
    "      The movies DataFrame, augmented to include a new column called 'tokens'.\n",
    "\n",
    "    \"\"\"\n",
    "    movies['tokens'] = movies['genres'].map(lambda x: tokenize_string(x))    \n",
    "    return movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize(movies):\n",
    "    \"\"\"\n",
    "    Append a new column to the movies DataFrame with header 'features'.\n",
    "    Each row will contain a csr_matrix of shape (1, num_features). Each\n",
    "    entry in this matrix will contain the tf-idf value of the term.\n",
    "    \n",
    "    tfidf(i, d) := tf(i, d) / max_k tf(k, d) * log10(N/df(i))\n",
    "    where:\n",
    "    i is a term\n",
    "    d is a document (movie)\n",
    "    tf(i, d) is the frequency of term i in document d\n",
    "    max_k tf(k, d) is the maximum frequency of any term in document d\n",
    "    N is the number of documents (movies)\n",
    "    df(i) is the number of unique documents containing term i\n",
    "\n",
    "    Params:\n",
    "      movies...The movies DataFrame\n",
    "    Returns:\n",
    "      A tuple containing:\n",
    "      - The movies DataFrame, which has been modified to include a column named 'features'.\n",
    "      - The vocab, a dict from term to int. Make sure the vocab is sorted alphabetically as in a2 (e.g., {'aardvark': 0, 'boy': 1, ...})\n",
    "    \"\"\"\n",
    "    vocab_set = set()\n",
    "    \n",
    "    for movie in movies['tokens']:\n",
    "        for token in movie:\n",
    "            vocab_set.add(token)\n",
    "    \n",
    "    vocab_list = list(vocab_set)\n",
    "    \n",
    "    df = Counter()\n",
    "    \n",
    "    for v in vocab_list:\n",
    "        for movie in movies['tokens']:\n",
    "            if v in movie:\n",
    "                df[v] += 1\n",
    "    \n",
    "    vocab_list.sort()\n",
    "    feature_list = []\n",
    "    vocab = defaultdict()\n",
    "    for index,value in enumerate(vocab_list):\n",
    "        vocab[value] = index\n",
    "\n",
    "    N = movies.shape[0]\n",
    "    for movie in movies['tokens']:\n",
    "        tf = Counter()\n",
    "        for tok in movie:\n",
    "            tf[tok] += 1\n",
    "        \n",
    "        max_k = sorted(tf.values(), key=lambda x: -x)[0]\n",
    "        data = []\n",
    "        col = []\n",
    "        row = []\n",
    "        for tok in tf:\n",
    "            if tok in vocab:\n",
    "                tfid = ( (tf[tok]/max_k) * (math.log10(N/df[tok])) )\n",
    "\n",
    "                col.append(vocab[tok])\n",
    "                data.append(tfid)\n",
    "                row.append(0)\n",
    "\n",
    "        \n",
    "        feature_list.append(csr_matrix((data, (row, col)), shape=(1, len(vocab))).toarray())\n",
    "    movies['features'] = feature_list\n",
    "    return (movies,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(ratings):\n",
    "    \"\"\"\n",
    "    Returns a random split of the ratings matrix into a training and testing set.\n",
    "    \"\"\"\n",
    "    test = set(range(len(ratings))[::1000])\n",
    "    train = sorted(set(range(len(ratings))) - test)\n",
    "    test = sorted(test)\n",
    "    return ratings.iloc[train], ratings.iloc[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two 1-d csr_matrices.\n",
    "    Each matrix represents the tf-idf feature vector of a movie.\n",
    "    Params:\n",
    "      a...A csr_matrix with shape (1, number_features)\n",
    "      b...A csr_matrix with shape (1, number_features)\n",
    "    Returns:\n",
    "      The cosine similarity, defined as: dot(a, b) / ||a|| * ||b||\n",
    "      where ||a|| indicates the Euclidean norm (aka L2 norm) of vector a.\n",
    "    \"\"\"\n",
    "\n",
    "    return (np.dot(a,np.transpose(b))/(np.linalg.norm(a)*np.linalg.norm(b)))[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions(movies, ratings_train, ratings_test):\n",
    "    \"\"\"\n",
    "    Using the ratings in ratings_train, predict the ratings for each\n",
    "    row in ratings_test.\n",
    "\n",
    "    To predict the rating of user u for movie i: Compute the weighted average\n",
    "    rating for every other movie that u has rated.  Restrict this weighted\n",
    "    average to movies that have a positive cosine similarity with movie\n",
    "    i. The weight for movie m corresponds to the cosine similarity between m\n",
    "    and i.\n",
    "\n",
    "    If there are no other movies with positive cosine similarity to use in the\n",
    "    prediction, use the mean rating of the target user in ratings_train as the\n",
    "    prediction.\n",
    "\n",
    "    Params:\n",
    "      movies..........The movies DataFrame.\n",
    "      ratings_train...The subset of ratings used for making predictions. These are the \"historical\" data.\n",
    "      ratings_test....The subset of ratings that need to predicted. These are the \"future\" data.\n",
    "    Returns:\n",
    "      A numpy array containing one predicted rating for each element of ratings_test.\n",
    "    \"\"\"\n",
    "    ratings_test_subset = ratings_test[['userId','movieId']]\n",
    "    ratings_train_subset = ratings_train[['userId','movieId','rating']]\n",
    "    \n",
    "    predicted_ratings = []\n",
    "    \n",
    "    for test_row in ratings_test_subset.itertuples():\n",
    "        \n",
    "        ratings_train_subset_forUserId = ratings_train_subset[ratings_train_subset['userId'] == test_row[1]]\n",
    "        weighted_rating = 0.0\n",
    "        size = ratings_train_subset_forUserId.shape[0]\n",
    "        rating_sum = 0.0\n",
    "        cos_sum = 0.0\n",
    "\n",
    "        b = movies[movies['movieId'] == test_row[2]]['features'].values[0]\n",
    "\n",
    "        for train_row in ratings_train_subset_forUserId.itertuples():\n",
    "            a = movies[movies['movieId'] == train_row[2]]['features'].values[0]\n",
    "            sim = cosine_sim(a, b)\n",
    "\n",
    "            rate = train_row[3]\n",
    "            \n",
    "            rating_sum = rating_sum + rate\n",
    "            weighted_rating = weighted_rating + (sim * rate)\n",
    "            cos_sum = cos_sum + sim\n",
    "\n",
    "        \n",
    "        if weighted_rating > 0:\n",
    "            predicted_ratings.append(weighted_rating/cos_sum)\n",
    "        else:\n",
    "            predicted_ratings.append(rating_sum/size)\n",
    "        \n",
    "            \n",
    "    return (np.array(predicted_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_absolute_error(predictions, ratings_test):\n",
    "    \"\"\"\n",
    "    Return the mean absolute error of the predictions.\n",
    "    \"\"\"\n",
    "    return np.abs(predictions - np.array(ratings_test.rating)).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    path = 'ml-latest-small'\n",
    "    ratings = pd.read_csv(path + os.path.sep + 'ratings.csv')\n",
    "    movies = pd.read_csv(path + os.path.sep + 'movies.csv')\n",
    "    movies = tokenize(movies)\n",
    "    movies, vocab = featurize(movies)\n",
    "    print('vocab:')\n",
    "    print(sorted(vocab.items())[:10])\n",
    "    ratings_train, ratings_test = train_test_split(ratings)\n",
    "    print('%d training ratings; %d testing ratings' % (len(ratings_train), len(ratings_test)))\n",
    "    predictions = make_predictions(movies, ratings_train, ratings_test)\n",
    "    print('error=%f' % mean_absolute_error(predictions, ratings_test))\n",
    "    print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
